name: Scrape SofaScore Sequential

on:
  workflow_dispatch:
    inputs:
      years:
        description: 'Years to scrape (comma-separated, e.g. 2025,2024,2023)'
        required: true
        default: '2025,2024,2023,2022,2021,2020,2019'
      max_events_per_run:
        description: 'Max events before saving and relaunching (prevents blocks)'
        required: false
        default: '5000'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install curl_cffi

      - name: Download previous checkpoints
        continue-on-error: true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          mkdir -p 04_sofascore_complete/data
          # Try to download latest checkpoints artifact from previous runs
          ARTIFACT_ID=$(gh api repos/${{ github.repository }}/actions/artifacts \
            --jq '[.artifacts[] | select(.name | startswith("checkpoints-")) | select(.expired == false)] | sort_by(.created_at) | last | .id' 2>/dev/null)

          if [ -n "$ARTIFACT_ID" ] && [ "$ARTIFACT_ID" != "null" ]; then
            echo "Downloading checkpoints from artifact $ARTIFACT_ID"
            gh api repos/${{ github.repository }}/actions/artifacts/$ARTIFACT_ID/zip > /tmp/checkpoints.zip 2>/dev/null
            unzip -o /tmp/checkpoints.zip -d 04_sofascore_complete/data/ 2>/dev/null || true
            echo "Checkpoints downloaded:"
            ls -la 04_sofascore_complete/data/checkpoint_*.json 2>/dev/null || echo "  None found"
          else
            echo "No previous checkpoints found"
          fi

      - name: Show IP
        run: curl -s ifconfig.me && echo ""

      - name: Scrape all years sequentially
        id: scrape
        run: |
          mkdir -p 04_sofascore_complete/data/raw

          python << 'PYEOF'
          import os, sys, json, time, logging, hashlib
          from pathlib import Path
          from datetime import date, timedelta
          from concurrent.futures import ThreadPoolExecutor, as_completed

          from curl_cffi import requests as cffi_requests

          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger(__name__)

          YEARS_STR = "${{ github.event.inputs.years }}"
          MAX_PER_RUN = int("${{ github.event.inputs.max_events_per_run }}" or "5000")
          BASE_URL = "https://api.sofascore.com/api/v1"
          MAX_WORKERS = 40
          BATCH_SIZE = 200
          TIMEOUT = 15
          IMPERSONATIONS = ["chrome110", "chrome116", "chrome120", "chrome123", "chrome124", "chrome131"]
          DATA_DIR = Path("04_sofascore_complete/data")
          GLOBAL_STATUS_FILE = DATA_DIR / "global_status.json"

          YEARS = [int(y.strip()) for y in YEARS_STR.split(',') if y.strip()]
          logger.info(f"Years to process: {YEARS}")
          logger.info(f"Max events per run: {MAX_PER_RUN}")

          _imp_counter = 0
          def get_imp():
              global _imp_counter
              imp = IMPERSONATIONS[_imp_counter % len(IMPERSONATIONS)]
              _imp_counter += 1
              return imp

          def http_get(url):
              for attempt in range(2):
                  try:
                      r = cffi_requests.get(url, impersonate=get_imp(), timeout=TIMEOUT)
                      if r.status_code == 200:
                          return r.json()
                      elif r.status_code == 404:
                          return None
                      elif r.status_code in (403, 429):
                          return {'__blocked__': True}
                  except:
                      pass
                  time.sleep(0.5)
              return None

          def discover_events(year):
              """Discover all finished events for a year via date API."""
              logger.info(f"Discovering events for {year}...")
              start_date = date(year, 1, 1)
              end_date = min(date(year, 12, 31), date.today() - timedelta(days=1))
              all_events = {}
              current = start_date
              days_total = (end_date - start_date).days + 1
              day_count = 0
              consecutive_blocks = 0

              while current <= end_date:
                  date_str = current.strftime("%Y-%m-%d")
                  url = f"{BASE_URL}/sport/football/scheduled-events/{date_str}"
                  data = http_get(url)

                  if data and data.get('__blocked__'):
                      consecutive_blocks += 1
                      if consecutive_blocks >= 3:
                          logger.warning(f"  3 consecutive blocks during discovery at {date_str}, stopping")
                          return all_events, True  # blocked
                      logger.warning(f"  Blocked at {date_str}, waiting 30s...")
                      time.sleep(30)
                      continue

                  consecutive_blocks = 0
                  if data:
                      events = data.get('events', [])
                      for evt in events:
                          status = evt.get('status', {}).get('type', '')
                          eid = evt.get('id')
                          if status == 'finished' and eid:
                              all_events[eid] = date_str

                  day_count += 1
                  if day_count % 30 == 0:
                      logger.info(f"  Dates: {day_count}/{days_total}, events: {len(all_events)}")

                  current += timedelta(days=1)
                  time.sleep(0.05)

              logger.info(f"  Total events {year}: {len(all_events)}")
              return all_events, False

          def scrape_event(event_id, date_str):
              result = {
                  'event_id': event_id,
                  'scraped_at': time.strftime('%Y-%m-%dT%H:%M:%S'),
                  'odds': None, 'statistics': None,
                  'lineups': None, 'incidents': None, 'blocked': False
              }
              endpoints = {
                  'odds': f'{BASE_URL}/event/{event_id}/odds/1/all',
                  'statistics': f'{BASE_URL}/event/{event_id}/statistics',
                  'lineups': f'{BASE_URL}/event/{event_id}/lineups',
                  'incidents': f'{BASE_URL}/event/{event_id}/incidents',
              }
              blocked = 0
              with ThreadPoolExecutor(max_workers=4) as ex:
                  futs = {ex.submit(http_get, url): name for name, url in endpoints.items()}
                  for f in as_completed(futs):
                      name = futs[f]
                      try:
                          data = f.result()
                          if data and isinstance(data, dict) and data.get('__blocked__'):
                              blocked += 1
                          else:
                              result[name] = data
                      except:
                          pass
              if blocked >= 2:
                  result['blocked'] = True
              return result

          def scrape_year(year, max_events):
              """Scrape details for a year. Returns (processed, blocked, year_done)."""
              raw_dir = DATA_DIR / "raw" / str(year)
              raw_dir.mkdir(parents=True, exist_ok=True)
              checkpoint_file = DATA_DIR / f"checkpoint_{year}.json"

              # Load checkpoint
              completed = set()
              cp = {'year': year, 'completed_events': [], 'total_processed': 0,
                    'total_odds': 0, 'total_stats': 0, 'total_lineups': 0, 'total_incidents': 0}
              if checkpoint_file.exists():
                  with open(checkpoint_file) as f:
                      cp = json.load(f)
                  completed = set(cp.get('completed_events', []))
                  logger.info(f"  Checkpoint {year}: {len(completed)} already done")

              # Discover events
              all_events, discovery_blocked = discover_events(year)
              if discovery_blocked and len(all_events) == 0:
                  return 0, True, False

              # Filter pending
              pending = [(eid, d) for eid, d in all_events.items() if eid not in completed]
              pending.sort(key=lambda x: x[1], reverse=True)

              if not pending:
                  logger.info(f"  Year {year}: all {len(completed)} events already done!")
                  return 0, False, True

              # Limit per run
              pending = pending[:max_events]
              logger.info(f"  Year {year}: {len(pending)} pending (of {len(all_events)} total, {len(completed)} done)")

              total = len(pending)
              i = 0
              start_time = time.time()
              was_blocked = False
              consecutive_block_batches = 0

              while i < total:
                  batch = pending[i:i+BATCH_SIZE]
                  blocked_count = 0

                  with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                      futs = {executor.submit(scrape_event, eid, d): (eid, d) for eid, d in batch}
                      for f in as_completed(futs):
                          eid, d = futs[f]
                          try:
                              result = f.result()
                              if result['blocked']:
                                  blocked_count += 1
                                  continue
                              filepath = raw_dir / f"{d}_{eid}.json"
                              with open(filepath, 'w') as fp:
                                  json.dump(result, fp, ensure_ascii=False)
                              completed.add(eid)
                              cp['total_processed'] += 1
                              if result.get('odds') and isinstance(result['odds'], dict) and result['odds'].get('markets'):
                                  cp['total_odds'] += 1
                              if result.get('statistics') and isinstance(result['statistics'], dict) and result['statistics'].get('statistics'):
                                  cp['total_stats'] += 1
                              if result.get('lineups') and isinstance(result['lineups'], dict):
                                  cp['total_lineups'] += 1
                              if result.get('incidents') and isinstance(result['incidents'], dict) and result['incidents'].get('incidents'):
                                  cp['total_incidents'] += 1
                          except Exception as e:
                              logger.error(f"Error {eid}: {e}")

                  # Save checkpoint
                  cp['completed_events'] = list(completed)
                  with open(checkpoint_file, 'w') as f:
                      json.dump(cp, f)

                  if blocked_count > len(batch) * 0.5:
                      consecutive_block_batches += 1
                      if consecutive_block_batches >= 2:
                          logger.warning(f"  BLOCKED 2x consecutive! Saving and stopping year {year}")
                          was_blocked = True
                          break
                      logger.warning(f"  Blocked batch ({blocked_count}/{len(batch)}), waiting 60s...")
                      time.sleep(60)
                      continue
                  else:
                      consecutive_block_batches = 0

                  i += BATCH_SIZE
                  elapsed = time.time() - start_time
                  rate = cp['total_processed'] / max(elapsed, 1)
                  logger.info(
                      f"  [{year}] [{min(i,total)}/{total}] Done: {cp['total_processed']} | "
                      f"Odds: {cp['total_odds']} | Stats: {cp['total_stats']} | "
                      f"{rate:.1f} evt/s"
                  )

              elapsed_min = (time.time() - start_time) / 60
              all_done = (len(completed) >= len(all_events))
              logger.info(f"  Year {year} finished in {elapsed_min:.1f} min | "
                         f"Processed: {cp['total_processed']} | Total done: {len(completed)}/{len(all_events)} | "
                         f"Blocked: {was_blocked}")
              return cp['total_processed'], was_blocked, all_done

          # ============================================================
          # MAIN: Process years sequentially
          # ============================================================
          global_processed = 0
          blocked_at_year = None
          years_done = []
          years_pending = []

          for year in YEARS:
              logger.info(f"\n{'='*60}")
              logger.info(f"PROCESSING YEAR {year}")
              logger.info(f"{'='*60}")

              processed, blocked, year_done = scrape_year(year, MAX_PER_RUN)
              global_processed += processed

              if year_done:
                  years_done.append(year)
              else:
                  years_pending.append(year)

              if blocked:
                  blocked_at_year = year
                  # Add remaining years as pending
                  idx = YEARS.index(year)
                  for remaining_year in YEARS[idx+1:]:
                      years_pending.append(remaining_year)
                  logger.warning(f"\nBLOCKED at year {year}! Will need relaunch.")
                  break

          # Save global status
          status = {
              'years_done': years_done,
              'years_pending': years_pending,
              'blocked_at_year': blocked_at_year,
              'total_processed_this_run': global_processed,
              'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S'),
              'needs_relaunch': len(years_pending) > 0
          }
          with open(GLOBAL_STATUS_FILE, 'w') as f:
              json.dump(status, f, indent=2)

          logger.info(f"\n{'='*60}")
          logger.info(f"RUN COMPLETE")
          logger.info(f"  Years done: {years_done}")
          logger.info(f"  Years pending: {years_pending}")
          logger.info(f"  Total processed: {global_processed}")
          logger.info(f"  Blocked: {blocked_at_year is not None}")
          logger.info(f"{'='*60}")

          # Exit with code 0 always (artifacts must be uploaded)
          PYEOF

      - name: Show results
        if: always()
        run: |
          echo "=== RESULTS ==="
          for year in $(echo "${{ github.event.inputs.years }}" | tr ',' ' '); do
            count=$(find "04_sofascore_complete/data/raw/$year/" -name "*.json" 2>/dev/null | wc -l)
            echo "  Year $year: $count JSONs"
            if [ -f "04_sofascore_complete/data/checkpoint_${year}.json" ]; then
              python -c "
          import json
          d=json.load(open('04_sofascore_complete/data/checkpoint_${year}.json'))
          print(f'    Checkpoint: {len(d.get(\"completed_events\",[]))} done, odds={d.get(\"total_odds\",0)}, stats={d.get(\"total_stats\",0)}')
          " 2>/dev/null
            fi
          done

          echo ""
          echo "=== GLOBAL STATUS ==="
          cat 04_sofascore_complete/data/global_status.json 2>/dev/null || echo "No status file"

      - name: Upload raw JSONs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: raw-jsons-sequential-${{ github.run_number }}
          path: 04_sofascore_complete/data/raw/
          retention-days: 30
          compression-level: 9
          include-hidden-files: false

      - name: Upload checkpoints
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: checkpoints-${{ github.run_number }}
          path: 04_sofascore_complete/data/checkpoint_*.json
          retention-days: 90

      - name: Upload global status
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: status-${{ github.run_number }}
          path: 04_sofascore_complete/data/global_status.json
          retention-days: 90

      - name: Auto-relaunch if needed
        if: always()
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if [ -f "04_sofascore_complete/data/global_status.json" ]; then
            NEEDS=$(python -c "import json; d=json.load(open('04_sofascore_complete/data/global_status.json')); print(d.get('needs_relaunch', False))")
            PENDING=$(python -c "import json; d=json.load(open('04_sofascore_complete/data/global_status.json')); print(','.join(str(y) for y in d.get('years_pending', [])))")

            if [ "$NEEDS" = "True" ] && [ -n "$PENDING" ]; then
              echo "Relaunching with pending years: $PENDING"
              gh workflow run scrape_sequential.yml \
                -f years="$PENDING" \
                -f max_events_per_run="${{ github.event.inputs.max_events_per_run }}"
              echo "Relaunch triggered!"
            else
              echo "All years complete! No relaunch needed."
            fi
          fi
